---
title: "Week 7 modules"
output: html_notebook
---

#Module 11 - Type I and Type II Error and Power


What is True	  What We Decide	  Result
H0	          H0	          Correctly ‘accept’ the null
H0	          HA          	Falsely reject the null (Type I error)
HA  	        H0	          Falsely ‘accept’ the null (Type II error)
HA          	HA          	Correctly reject the null
alpha - frequency of type 1 error (i.e. 5%, a=0.05)
beta - frequency of type 2 error (not typically known)
##Type I Error
Multiple testing problem - if rate of type 1 error is 1 in 20, repeating multiple t-tests increases the rate of having a type 1 error in the tests. 

```{r}
typeI <- function(mu0, sigma, n, alternative = "two.tailed", alpha = 0.05, k = 10000) {
}
```

```{r}
typeI <- function(mu0, sigma, n, alternative = "two.tailed", alpha = 0.05, k = 1000) {
    p <- rep(NA, k)  # sets up a vector of empty p values
    for (i in 1:k) {
        # sets up a loop to run k simulations
        x <- rnorm(n = n, mean = mu0, sd = sigma)  # draws a sample from our distribution
        m <- mean(x)  # calculates the mean
        s <- sd(x)  # calculates the standard deviation
        z <- (m - mu0)/(s/sqrt(n))  # calculates the T statistic for the sample drawn from the null distribution relative to the null distribution
        # alternatively use t <- (m-mu0)/(s/sqrt(n))
        if (alternative == "less") {
            p[i] <- pnorm(z, lower.tail = TRUE)  # calculates the associated p value
            # alternatively, use p[i] <- pt(t,df=n-1,lower.tail=TRUE)
        }
        if (alternative == "greater") {
            p[i] <- pnorm(z, lower.tail = FALSE)  # calculates the associated p value
            # alternatively, use p[i] <- pt(t,df=n-1,lower.tail=FALSE)
        }
        if (alternative == "two.tailed") {
            if (z > 0) 
                {
                  p[i] <- 2 * pnorm(z, lower.tail = FALSE)
                }  # alternatively, use if (t > 0) {p[i] <- pt(t,df=n-1,lower.tail=FALSE)}
            if (z < 0) 
                {
                  p[i] <- 2 * pnorm(z, lower.tail = TRUE)
                }  # alternatively, use if (t < 0) {p[i] <- pt(t,df=n-1,lower.tail=TRUE)}
        }
    }
    
    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n), 
        main = paste("Sampling Distribution Under the Null Hypothesis\nType I error rate from simulation = ", 
            length(p[p < alpha])/k, sep = ""), xlab = "x", ylab = "Pr(x)", col = "red", 
        xlim = c(mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n)), ylim = c(0, 
            dnorm(mu0, mu0, sigma/sqrt(n))))
    abline(h = 0)
    
    if (alternative == "less") {
        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n), 
            to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), 
            mu0 - qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 
            4 * sigma/sqrt(n), to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), 
            length.out = 100), mean = mu0, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        q <- pnorm(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - 
            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))
    }
    if (alternative == "greater") {
        polygon(cbind(c(mu0 + qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 + 
            qnorm(1 - alpha) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), 
            length.out = 100), mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + 
            qnorm(1 - alpha) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), 
            length.out = 100), mean = mu0, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        q <- pnorm(mu0 + 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - 
            pnorm(mu0 + qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))
    }
    if (alternative == "two.tailed") {
        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n), 
            to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100), 
            mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 
            4 * sigma/sqrt(n), to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), 
            length.out = 100), mean = mu0, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        polygon(cbind(c(mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 + 
            qnorm(1 - alpha/2) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), 
            length.out = 100), mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + 
            qnorm(1 - alpha/2) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), 
            length.out = 100), mean = mu0, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        q <- pnorm(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - 
            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) + 
            pnorm(mu0 + 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - 
            pnorm(mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))
    }
    # print(round(q,digits=3)) # this prints area in the shaded portion(s) of
    # the curve
    return(length(p[p < alpha])/k)  # returns the proportion of simulations where p < alpha
}
```
```{r}
eI <- typeI(mu0 = -3, sigma = 2, n = 5000, alternative = "greater", alpha = 0.05)
```

```{r}
eI <- typeI(mu0 = 5, sigma = 2, n = 1000, alternative = "less", alpha = 0.01)
```
 Bonferroni correction
```{r}
alpha <- 0.05
pvals <- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.23)
sig <- pvals <= alpha/length(pvals)
sig  # first 3 values are less than the adjusted alpha
```


Benjamini & Hochberg correction
```{r}
library(ggplot2)
alpha <- 0.05
psig <- NULL
pvals <- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.27)
for (i in 1:length(pvals)) {
    psig[i] <- alpha * i/length(pvals)
}
d <- data.frame(cbind(rank = c(1:10), pvals, psig))
p <- ggplot(data = d, aes(x = rank, y = pvals)) + geom_point() + geom_line(aes(x = rank, 
    y = psig))
p

```

```{r}
sig <- pvals <= psig  # vector of significant pvalues
sig  # first 5 values are less than the adjusted alpha
```

```{r}
sig <- p.adjust(pvals, method = "bonferroni") <= 0.05
sig  # first 3 adjusted p values are less alpha
```

```{r}
sig <- p.adjust(pvals, method = "BH") <= 0.05
sig  # first 4 adjusted p values are less alpha
```
##Type II Error
```{r}
typeII <- function(mu0, muA, sigma, n, alternative = "two.tailed", alpha = 0.05, 
    k = 1000) {
    p <- rep(NA, k)  # sets up a vector of empty p values
    for (i in 1:k) {
        x <- rnorm(n = n, mean = muA, sd = sigma)  # draw from Ha
        m <- mean(x)
        s <- sd(x)
        z <- (m - mu0)/(s/sqrt(n))  # calculates the Z statistic for the sample drawn from Ha relative to the null distribution
        if (alternative == "less") {
            p[i] <- pnorm(z, lower.tail = TRUE)  # calculates the associated p value
            hyp <- "muA < mu0"
        }
        if (alternative == "greater") {
            p[i] <- pnorm(z, lower.tail = FALSE)
            hyp <- "muA > mu0"
        }
        if (alternative == "two.tailed") {
            if (z > 0) {
                p[i] <- 2 * pnorm(z, lower.tail = FALSE)
            }
            if (z < 0) {
                p[i] <- 2 * pnorm(z, lower.tail = TRUE)
            }
            hyp <- "muA ≠ mu0"
        }
    }
    
    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n), 
        main = paste("Sampling Distributions Under the Null (red)\nand Alternative Hypotheses (blue)\nType II error rate from simulation = ", 
            length(p[p >= alpha])/k, sep = ""), xlab = "x", ylab = "Pr(x)", 
        col = "red", xlim = c(min(c(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n))), 
            max(c(mu0 + 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), ylim = c(0, 
            max(c(dnorm(mu0, mu0, sigma/sqrt(n))), dnorm(muA, muA, sigma/sqrt(n)))))
    
    curve(dnorm(x, muA, sigma/sqrt(n)), muA - 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n), 
        col = "blue", add = TRUE)
    abline(h = 0)
    
    if (alternative == "less") {
        polygon(cbind(c(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 - 
            qnorm(1 - alpha) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), 
            length.out = 100), muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(mu0 - 
            qnorm(1 - alpha) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), 
            length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        abline(v = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), col = "black", lty = 3, 
            lwd = 2)
    }
    
    if (alternative == "greater") {
        polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n), 
            to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), 
            mu0 + qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA - 
            4 * sigma/sqrt(n), to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), 
            length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = "black", 
            col = "grey")
        abline(v = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), col = "black", lty = 3, 
            lwd = 2)
    }
    
    if (alternative == "two.tailed") {
        abline(v = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), col = "black", 
            lty = 3, lwd = 2)
        abline(v = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), col = "black", 
            lty = 3, lwd = 2)
        
        if (z > 0) {
            # greater
            polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n), 
                to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100), 
                mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA - 
                4 * sigma/sqrt(n), to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), 
                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = "black", 
                col = "grey")
        }
        
        # less
        if (z < 0) {
            polygon(cbind(c(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 - 
                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), 
                length.out = 100), muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(mu0 - 
                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), 
                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = "black", 
                col = "grey")
        }
    }
    
    return(length(p[p >= alpha])/k)
}
```

```{r}
eII <- typeII(mu0 = 2, muA = 4, sigma = 3, n = 6, alternative = "greater")  # Ha > H0
```
Questions: is n (Type II error function), refering to number in each sample, or all together
```{r}
eII <- typeII(mu0 = 5, muA = 2, sigma = 4, n = 18, alternative = "less")  # Ha < H0
```
##Power
Power is the probability of correctly rejecting a null hypothesis that is untrue. For a test that has a Type II error rate of ββ, the statistical power is defined, simply, as 1−β1−β. Power values of 0.8 or greater are conventionally considered to be high. Power for any given test depends on the difference between μμ between groups/treatments, αα, nn, and σσ.

##Effect Size
Generally speaking, an effect size is a quantitative measure of the strength of a phenomenon. Here, we are interested in comparing two sample means, and the most common way to describe the effect size is as a standardized difference between the means of the groups being compared. In this case, we divide the difference between the means by the standard deviation: |(μ0|(μ0 - μA)|/σμA)|/σ. This results in a scaleless measure. Conventionally, effect sizes of 0.2 or less are considered to be low and of 0.8 or greater are considered to be high.

###Graphical Depiction of Power and Effect Size
```{r}
library(ggplot2)
library(manipulate)
power.plot <- function(sigma, muA, mu0, n, alpha, alternative = "two.tailed") {
    pow <- 0
    z <- (muA - mu0)/(sigma/sqrt(n))
    g <- ggplot(data.frame(mu = c(min(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n)), 
        max(mu0 + 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), aes(x = mu)) + 
        ggtitle("Explore Power for Z Test")
    g <- g + ylim(c(0, max(dnorm(mu0, mu0, sigma/sqrt(n)) + 0.1, dnorm(muA, 
        muA, sigma/sqrt(n)) + 0.1)))
    g <- g + stat_function(fun = dnorm, geom = "line", args = list(mean = mu0, 
        sd = sigma/sqrt(n)), size = 1, col = "red", show.legend = TRUE)
    g <- g + stat_function(fun = dnorm, geom = "line", args = list(mean = muA, 
        sd = sigma/sqrt(n)), size = 1, col = "blue", show.legend = TRUE)
    
    if (alternative == "greater") {
        if (z > 0) {
            xcrit = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)
            g <- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0, 
                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 
                0.025), size = 0.5, linetype = 3)
            g <- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit, 
                to = muA + 4 * sigma/sqrt(n), length.out = 100), muA + 4 * sigma/sqrt(n)), 
                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), 
                  length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), 
                aes(x = x, y = y), fill = "blue", alpha = 0.5)
            pow <- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit, 
                muA, sigma/sqrt(n))
        }
    }
    if (alternative == "less") {
        if (z < 0) {
            xcrit = mu0 - qnorm(1 - alpha) * sigma/sqrt(n)
            g <- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0, 
                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 
                0.025), size = 0.5, linetype = 3)
            g <- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n), 
                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 100), 
                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, 
                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, 
                y = y), fill = "blue", alpha = 0.5)
            pow <- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n), 
                muA, sigma/sqrt(n))
        }
    }
    if (alternative == "two.tailed") {
        if (z > 0) {
            xcrit = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n)
            g <- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0, 
                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 
                0.025), size = 0.5, linetype = 3)
            g <- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit, 
                to = muA + 4 * sigma/sqrt(n), length.out = 100), muA + 4 * sigma/sqrt(n)), 
                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), 
                  length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), 
                aes(x = x, y = y), fill = "blue", alpha = 0.5)
            pow <- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit, 
                muA, sigma/sqrt(n))
        }
        if (z < 0) {
            xcrit = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n)
            g <- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0, 
                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 
                0.025), size = 0.5, linetype = 3)
            g <- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n), 
                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 100), 
                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, 
                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, 
                y = y), fill = "blue", alpha = 0.5)
            pow <- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n), 
                muA, sigma/sqrt(n))
        }
    }
    g <- g + annotate("text", x = max(mu0, muA) + 2 * sigma/sqrt(n), y = max(dnorm(mu0, 
        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), 
        label = paste("Effect Size = ", round((muA - mu0)/sigma, digits = 3), 
            "\nPower = ", round(pow, digits = 3), sep = ""))
    g <- g + annotate("text", x = min(mu0, muA) - 2 * sigma/sqrt(n), y = max(dnorm(mu0, 
        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), 
        label = "Red = mu0\nBlue = muA")
    g
}
```

```{r}
manipulate(power.plot(sigma, muA, mu0, n, alpha, alternative), sigma = slider(1, 
    10, step = 1, initial = 4), muA = slider(-10, 10, step = 1, initial = 2), 
    mu0 = slider(-10, 10, step = 1, initial = 0), n = slider(1, 50, step = 1, 
        initial = 16), alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05), 
    alternative = picker("two.tailed", "greater", "less"))
#use bootstrapping to help find the power. 
```

```{r}
library(ggplot2)
library(manipulate)
power.test <- function(mu0, muA, sigma, alpha = 0.05, type, alternative) {
    p <- 0
    for (i in 2:200) {
        x <- power.t.test(n = i, delta = abs(muA - mu0), sd = sigma, sig.level = alpha, 
            power = NULL, type = type, alternative = alternative)
        p <- c(p, x$power)
    }
    d <- data.frame(cbind(1:200, p, 1 - p))
    critn <- 0
    for (i in 1:199) {
        if (p[i] < 0.8 && p[i + 1] >= 0.8) {
            critn <- i + 1
        } else {
            critn <- critn
        }
    }
    names(d) <- c("n", "power", "beta")
    g <- ggplot(data = d) + xlab("sample size n") + ylab("Type II Error Rate, Beta  (Red)\nand\nPower, 1-Beta (Blue)") + 
        ggtitle("Power for T Tests\n(assuming equal n and variance across the two groups)") + 
        ylim(0, 1) + geom_point(aes(x = n, y = power), colour = "blue", alpha = 1/2) + 
        geom_line(aes(x = n, y = power), colour = "blue", alpha = 1/2) + geom_line(aes(x = n, 
        y = 0.8), colour = "red", lty = 3) + geom_point(aes(x = n, y = beta), 
        colour = "red", alpha = 1/2) + geom_line(aes(x = n, y = beta), colour = "red", 
        alpha = 1/2) + geom_linerange(aes(x = critn, ymin = 0, ymax = 0.8), 
        colour = "blue", alpha = 1/4) + annotate("text", x = 150, y = 0.5, label = paste("Effect Size = ", 
        round(abs(mu0 - muA)/sigma, digits = 3), "\nCritical n = ", critn, sep = ""))
    print(g)
}
```

```{r}
manipulate(power.test(mu0, muA, sigma, alpha, type, alternative), mu0 = slider(-10, 
    10, initial = 3, step = 1), muA = slider(-10, 10, initial = 0, step = 1), 
    sigma = slider(1, 10, initial = 3, step = 1), alpha = slider(0.01, 0.1, 
        initial = 0.05, step = 0.01), alternative = picker("two.sided", "one.sided"), 
    type = picker("two.sample", "one.sample", "paired"))
```

#Module 12 -- Only use model 1 (OLS) if you are really secure on the measurement of x 
install.packages("lmodel2")
install.packages("gridExtra")


##Introduction to Linear Regression
##Covariance and Correlation

```{r}
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/zombies.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r}
plot(data = d, height ~ weight)
```
CHALLENGE:

What is the covariance between zombie weight and zombie height? What does it mean if the covariance is positive versus negative? Does it matter if you switch the order of the two variables?
```{r}
w <- d$weight
h <- d$height
n <- length(w)  # or length(h)
cov_wh <- sum((w - mean(w)) * (h - mean(h)))/(n - 1)
cov_wh
```

```{r}
cov(w, h)
```
CHALLENGE:

Calculate the correlation between zombie height and weight.
```{r}
sd_w <- sd(w)
sd_h <- sd(h)
cor_wh <- cov_wh/(sd_w * sd_h)
cor_wh
```

```{r}
cor(w, h)
```
This formulation of the correlation coefficient is referred to as Pearson’s product-moment correlation coefficient and is often abbreviated as ρρ.

There are other, nonparametric forms of the correlation coefficient we might also calculate
```{r}
cor(w, h, method = "kendall")
```

```{r}
cor(w, h, method = "spearman")
```
##Regression
```{r}
y <- h - mean(h)
x <- w - mean(w)
z <- data.frame(cbind(x, y))
g <- ggplot(data = z, aes(x = x, y = y)) + geom_point()
g
```

```{r}
slope.test <- function(beta1) {
    g <- ggplot(data = z, aes(x = x, y = y))
    g <- g + geom_point()
    g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue", 
        alpha = 1/2)
    ols <- sum((y - beta1 * x)^2)
    g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ", 
        round(ols, 3)))
    g
}
```

```{r}
manipulate(slope.test(beta1), beta1 = slider(-1, 1, initial = 0, step = 0.005))

```

```{r}
beta1 <- cor(w, h) * (sd(h)/sd(w))
beta1
```

```{r}
beta1 <- cov(w, h)/var(w)
beta1
```
```{r}
beta1 <- sum((h - mean(h)) * (w - mean(w)))/sum((w - mean(w))^2)
beta1
```

```{r}
beta0 <- mean(h) - beta1 * mean(w)
beta0
```
## The lm Function
```{r}
m <- lm(height ~ weight, data = d)
m
```

```{r}
names(m)
m$coefficients
```

```{r}
head(m$model)
```

```{r}
g <- ggplot(data = d, aes(x = weight, y = height))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```

```{r}
library(lmodel2)  # load the lmodel2 package
# Run the regression
mII <- lmodel2(height ~ weight, data = d, range.y = "relative", range.x = "relative", 
    nperm = 1000)
mII
```
```{r}
plot(mII, "OLS")
```

```{r}
plot(mII, "RMA")
```

```{r}
plot(mII, "SMA")
```

```{r}
plot(mII, "MA")
```

```{r}
mI <- lm(height ~ weight, data = d)
summary(mI)
```

```{r}
par(mfrow = c(1, 2))
plot(mII, main = "lmodel2() OLS")
plot(data = d, height ~ weight, main = "lm()")
abline(mI)
```

CHALLENGE:

Using the zombies dataset, work with a partner to…

Plot zombie height as a function of age
Derive by hand the ordinary least squares regression coefficients β1
 and β0
 for these data.
Confirm that you get the same results using the lm() function
Repeat the analysis above for males and females separately. Do your regression coefficients differ? How might you determine this?

```{r}
plot(data = d, height ~ age)
```

```{r}
beta1 <- cor(d$height, d$age) * sd(d$height)/sd(d$age)
beta1
```

```{r}
beta0 <- mean(d$height) - beta1 * mean(d$age)
beta0
```

```{r}
m <- lm(height ~ age, data = d)
m
```
##Statistical Inference in Regression
```{r}
m <- lm(data = d, height ~ weight)
summary(m)
```
In our regression, ~69% of the variation in zombie height is explained by zombie weight.
```{r}
t <- coef(summary(m))
t <- data.frame(unlist(t))
colnames(t) <- c("Est", "SE", "t", "p")
t
```
```{r}
t$calct <- (t$Est - 0)/t$SE
t$calcp <- 2 * pt(t$calct, df = 998, lower.tail = FALSE)  # x2 because is 2-tailed test
t
```

```{r}
ci <- confint(m, level = 0.95)  # using the results of lm()
ci
```
##Interpreting Regression Coefficients and Prediction
CHALLENGE:

If zombie weight is measured in pounds and zombie height is measured in inches, what is the expected height of a zombie weighing 150 pounds?
What is the predicted difference in height between a zombie weighing 180 and 220 pounds?
```{r}
beta0 <- t$Est[1]
beta1 <- t$Est[2]
h_hat <- beta1 * 150 + beta0
h_hat
```

```{r}
h_hat_difference <- (beta1 * 220 + beta0) - (beta1 * 180 + beta0)
h_hat_difference
```

```{r}
m <- lm(data = d, height ~ weight)
h_hat <- predict(m, newdata = data.frame(weight = d$weight))
df <- data.frame(cbind(d$weight, d$height, h_hat))
names(df) <- c("x", "y", "yhat")
head(df)
```

```{r}
g <- ggplot(data = df, aes(x = x, y = yhat))
g <- g + geom_point(color="blue")
g <- g + geom_point(aes(x = x, y = y), colour = "red")
g <- g + geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
g

```

```{r}
ci <- predict(m, newdata = data.frame(weight = 150), interval = "confidence", 
    level = 0.95)  # for a single value
ci
```

```{r}
ci <- predict(m, newdata = data.frame(weight = d$weight), interval = "confidence", 
    level = 0.95)  # for a vector of values
head(ci)
```

```{r}
df <- cbind(df, ci)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")
head(df)
```

```{r}
g <- ggplot(data = df, aes(x = x, y = y))
g <- g + geom_point(alpha = 1/2)
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g
```

```{r}
pi <- predict(m, newdata = data.frame(weight = 150), interval = "prediction", 
    level = 0.95)  # for a single value
pi
```

```{r}
pi <- predict(m, newdata = data.frame(weight = d$weight), interval = "prediction", 
    level = 0.95)  # for a vector of values
head(pi)
```

```{r}
df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(df)
```

```{r}
g <- g + geom_line(data = df, aes(x = x, y = PIlwr), colour = "red")
g <- g + geom_line(data = df, aes(x = x, y = PIupr), colour = "red")
g
```
CHALLENGE:

Construct a linear model for the regression of zombie height on age and predict the mean height, the 95% confidence interval (CI) around the predicted mean height, and the 95% prediction interval (PI) around that mean for a vector of zombie ages, v <- seq(from=10, to=30, by=1). Then, plot your points, your regression line, and lines for the lower and upper limits of the CI and of the PI.
```{r}
v <- seq(from = 10, to = 30, by = 1)
m <- lm(data = d, height ~ age)
ci <- predict(m, newdata = data.frame(age = v), interval = "confidence", level = 0.95)
pi <- predict(m, newdata = data.frame(age = v), interval = "prediction", level = 0.95)
plot(data = d, height ~ age)
lines(x = v, y = ci[, 1], col = "black")
lines(x = v, y = ci[, 2], col = "blue")
lines(x = v, y = ci[, 3], col = "blue")
lines(x = v, y = pi[, 2], col = "red")
lines(x = v, y = pi[, 3], col = "red")
```

```{r}
# or
require(gridExtra)
require(ggplot2)
df <- data.frame(cbind(v, ci, pi))
names(df) <- c("age", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr")
head(df)
```

```{r}
g1 <- ggplot(data = d, aes(x = age, y = height))
g1 <- g1 + geom_point(alpha = 1/2)
g1 <- g1 + geom_line(data = df, aes(x = v, y = CIfit), colour = "black", lwd = 1)
g1 <- g1 + geom_line(data = df, aes(x = v, y = CIlwr), colour = "blue")
g1 <- g1 + geom_line(data = df, aes(x = v, y = CIupr), colour = "blue")
g1 <- g1 + geom_line(data = df, aes(x = v, y = PIlwr), colour = "red")
g1 <- g1 + geom_line(data = df, aes(x = v, y = PIupr), colour = "red")
g2 <- ggplot(data = d, aes(x = age, y = height))
g2 <- g2 + geom_point(alpha = 1/2)
g2 <- g2 + geom_smooth(method = "lm", formula = y ~ x)
grid.arrange(g1, g2, ncol = 2)
```
Again, here the CI band shows where the mean height is expected to fall in 95% of samples and the PI band shows where the individual points are expected to fall 95% of the time.


##Residuals

If there is structure in residuals - it means there is a variable that needs to be measured becuase its having an effect on the variables you have measured.
```{r}

```


